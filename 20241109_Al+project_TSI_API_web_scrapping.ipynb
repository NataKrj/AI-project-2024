{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjuZAHFASKx7/vY2Xmaj+3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/SergejsKopils/20241109_Al%2Bproject_TSI_API_web_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORK GOOD** google 1 page"
      ],
      "metadata": {
        "id": "v3sAnXkS6mIb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkf_7i-F6YwQ"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'\n",
        "google_cse_id = 'xxx'\n",
        "openai.api_key = 'xxx'\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = [\n",
        "    \"court\", \"criminal case\", \"accusation\", \"crime\", \"corruption\", \"penalty\",\n",
        "    \"investigation\", \"insolvency\", \"debt\", \"violation\", \"arrested\", \"sanctions\",\n",
        "    \"litigation\", \"shell company\", \"blackmail\"\n",
        "]\n",
        "\n",
        "# Function to perform Google search\n",
        "def google_search(search_term, api_key, cse_id, **kwargs):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
        "    return res.get('items', [])\n",
        "\n",
        "# Function to extract text from a URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = ' '.join(p.text for p in soup.find_all('p'))\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# Function to check if text contains any keywords\n",
        "def contains_keywords(text, keywords):\n",
        "    matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n",
        "    return \", \".join(matched_keywords) if matched_keywords else \"No match\"\n",
        "\n",
        "# Function to analyze text with GPT-4\n",
        "def analyze_text_with_gpt(text, company_name):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Determine if the following text is related to {company_name}.\"},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except openai.error.OpenAIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"Analysis failed.\"\n",
        "\n",
        "# Main process\n",
        "company_name = 'SwedBank'\n",
        "results = google_search(company_name, google_api_key, google_cse_id, num=10)\n",
        "\n",
        "# Create DataFrame to store results\n",
        "data = pd.DataFrame(columns=['company', 'url', 'extracted_text', 'gpt_analysis', 'related_keywords'])\n",
        "\n",
        "for result in results:\n",
        "    title = result['title']\n",
        "    link = result['link']\n",
        "\n",
        "    # Extract text from each URL\n",
        "    extracted_text = extract_text_from_url(link)\n",
        "\n",
        "    # Analyze extracted text with GPT-4 to determine relevance to the company\n",
        "    if extracted_text:\n",
        "        gpt_analysis = analyze_text_with_gpt(extracted_text, company_name)\n",
        "        related_keywords = contains_keywords(extracted_text, keywords)\n",
        "    else:\n",
        "        gpt_analysis = \"No text extracted.\"\n",
        "        related_keywords = \"No text extracted.\"\n",
        "\n",
        "    # Create a new row as a DataFrame and append it to the main DataFrame\n",
        "    new_row = pd.DataFrame({\n",
        "        'company': [company_name],\n",
        "        'url': [link],\n",
        "        'extracted_text': [extracted_text],\n",
        "        'gpt_analysis': [gpt_analysis],\n",
        "        'related_keywords': [related_keywords]  # Add matched keywords\n",
        "    })\n",
        "    data = pd.concat([data, new_row], ignore_index=True)  # Concatenate new row\n",
        "\n",
        "    # Delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust delay if necessary\n",
        "\n",
        "# Save the results to a CSV file\n",
        "data.to_csv('company_analysis_results.csv', index=False)\n",
        "print(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WORK GOOD** google 10 page"
      ],
      "metadata": {
        "id": "WQhnNuE56p6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from googleapiclient.discovery import build\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import openai\n",
        "import time\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'\n",
        "google_cse_id = 'xxx'\n",
        "openai.api_key = 'xxx'\n",
        "\n",
        "# Keywords to search for\n",
        "keywords = [\n",
        "    \"court\", \"criminal case\", \"accusation\", \"crime\", \"corruption\", \"penalty\",\n",
        "    \"investigation\", \"insolvency\", \"debt\", \"violation\", \"arrested\", \"sanctions\",\n",
        "    \"litigation\", \"shell company\", \"blackmail\"\n",
        "]\n",
        "\n",
        "# Function to perform Google search and get multiple pages\n",
        "def google_search(search_term, api_key, cse_id, num_pages=10):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    results = []\n",
        "\n",
        "    for page in range(num_pages):\n",
        "        start_index = page * 10 + 1  # 1, 11, 21, ... for each page\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        if 'items' in res:\n",
        "            results.extend(res['items'])\n",
        "        time.sleep(1)  # Avoid rate limits by adding a delay\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to extract text from a URL\n",
        "def extract_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            text = ' '.join(p.text for p in soup.find_all('p'))\n",
        "            return text\n",
        "        else:\n",
        "            print(f\"Failed to fetch {url} with status code {response.status_code}\")\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Request failed: {e}\")\n",
        "    return \"\"\n",
        "\n",
        "# Function to check if text contains any keywords\n",
        "def contains_keywords(text, keywords):\n",
        "    matched_keywords = [kw for kw in keywords if kw.lower() in text.lower()]\n",
        "    return \", \".join(matched_keywords) if matched_keywords else \"No match\"\n",
        "\n",
        "# Function to analyze text with GPT-4\n",
        "def analyze_text_with_gpt(text, company_name):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": f\"Determine if the following text is related to {company_name}.\"},\n",
        "                {\"role\": \"user\", \"content\": text}\n",
        "            ],\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response['choices'][0]['message']['content'].strip()\n",
        "    except openai.error.OpenAIError as e:\n",
        "        print(f\"OpenAI API error: {e}\")\n",
        "        return \"Analysis failed.\"\n",
        "\n",
        "# Main process\n",
        "company_name = 'SWEDBANK BALTICS, AS'\n",
        "results = google_search(company_name, google_api_key, google_cse_id, num_pages=10)\n",
        "\n",
        "# Create DataFrame to store results\n",
        "data = pd.DataFrame(columns=['company', 'url', 'extracted_text', 'gpt_analysis', 'related_keywords'])\n",
        "\n",
        "for result in results:\n",
        "    title = result['title']\n",
        "    link = result['link']\n",
        "\n",
        "    # Extract text from each URL\n",
        "    extracted_text = extract_text_from_url(link)\n",
        "\n",
        "    # Analyze extracted text with GPT-4 to determine relevance to the company\n",
        "    if extracted_text:\n",
        "        gpt_analysis = analyze_text_with_gpt(extracted_text, company_name)\n",
        "        related_keywords = contains_keywords(extracted_text, keywords)\n",
        "    else:\n",
        "        gpt_analysis = \"No text extracted.\"\n",
        "        related_keywords = \"No text extracted.\"\n",
        "\n",
        "    # Create a new row as a DataFrame and append it to the main DataFrame\n",
        "    new_row = pd.DataFrame({\n",
        "        'company': [company_name],\n",
        "        'url': [link],\n",
        "        'extracted_text': [extracted_text],\n",
        "        'gpt_analysis': [gpt_analysis],\n",
        "        'related_keywords': [related_keywords]  # Add matched keywords\n",
        "    })\n",
        "    data = pd.concat([data, new_row], ignore_index=True)  # Concatenate new row\n",
        "\n",
        "    # Delay to avoid rate limits\n",
        "    time.sleep(1)  # Adjust delay if necessary\n",
        "\n",
        "# Save the results to a CSV file\n",
        "data.to_csv('company_analysis_results.csv', index=False)\n",
        "print(data)\n"
      ],
      "metadata": {
        "id": "tlIH8jO76tXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
