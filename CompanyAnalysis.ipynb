{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/main/CompanyAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install pandas\n",
        "!pip install fuzzywuzzy\n",
        "!pip install selenium\n",
        "!pip install spacy nltk\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install rapidfuzz\n",
        "!pip install swifter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFZ4Whe53TKl",
        "outputId": "9bee58b2-c539-4921-80cd-483f1a766038"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.12.14)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32009 sha256=f2fb12d7fd15cefe77e8eff357e55a97395d1c113bd0732c2b8b1535c6ecc731\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.28.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.28.0-py3-none-any.whl (486 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.3/486.3 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.28.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz\n",
            "Successfully installed rapidfuzz-3.11.0\n",
            "Collecting swifter\n",
            "  Downloading swifter-1.4.0.tar.gz (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.6.6 in /usr/local/lib/python3.10/dist-packages (from swifter) (5.9.5)\n",
            "Requirement already satisfied: dask>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from dask[dataframe]>=2.10.0->swifter) (2024.10.0)\n",
            "Requirement already satisfied: tqdm>=4.33.0 in /usr/local/lib/python3.10/dist-packages (from swifter) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (24.2)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (8.5.0)\n",
            "Collecting dask-expr<1.2,>=1.1 (from dask[dataframe]>=2.10.0->swifter)\n",
            "  Downloading dask_expr-1.1.21-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->swifter) (2024.2)\n",
            "INFO: pip is looking at multiple versions of dask-expr to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading dask_expr-1.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.18-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading dask_expr-1.1.16-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: pyarrow>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from dask-expr<1.2,>=1.1->dask[dataframe]>=2.10.0->swifter) (17.0.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (3.21.0)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask>=2.10.0->dask[dataframe]>=2.10.0->swifter) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->swifter) (1.17.0)\n",
            "Downloading dask_expr-1.1.16-py3-none-any.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.2/243.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: swifter\n",
            "  Building wheel for swifter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for swifter: filename=swifter-1.4.0-py3-none-any.whl size=16507 sha256=dccbbcbe26ee7b6a7fa455d8ba21d1af677ac29c32e85d719788fe9aff35dcc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/cf/51/0904952972ee2c7aa3709437065278dc534ec1b8d2ad41b443\n",
            "Successfully built swifter\n",
            "Installing collected packages: dask-expr, swifter\n",
            "Successfully installed dask-expr-1.1.16 swifter-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "import random\n",
        "import spacy\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Load the file\n",
        "file_path = 'BELGIUM_companies.csv'\n",
        "df = pd.read_csv(file_path, encoding = 'utf-8' , low_memory=False)\n",
        "company_names = df['OriginalCompanyName'][0:500].tolist()\n",
        "\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"offshore\",\n",
        "    \"criminal case\", \"arrested\", \"fraud\", \"money laundering\",\n",
        "    \"embezzlement\", \"terrorism financing\", \"bribery\", \"tax evasion\",\n",
        "    \"illicit funds\", \"smuggling\", \"seized assets\", \"fines\",\n",
        "    \"indictment\", \"prosecuted\", \"wanted\", \"scam\", \"scandal\"\n",
        "]\n",
        "\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\",\n",
        "    \"insolvency\", \"violation\", \"debt\", \"blackmail\", \"lawsuit\",\n",
        "    \"default\", \"litigation\", \"settlement\", \"audit\", \"suspicious\",\n",
        "    \"foreclosure\", \"dispute\", \"breach\", \"illegal transaction\",\n",
        "    \"arbitration\", \"compliance failure\", \"tax fraud\"\n",
        "]\n",
        "\n",
        "keywords_score_minus_1 = [\"stock\"]  # Negative scoring\n",
        "\n",
        "score_no_words = 0\n",
        "\n",
        "# User-Agent Rotation\n",
        "user_agents = [\n",
        "\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 13_2 like Mac OS X)\"\n",
        "]\n",
        "\n",
        "def random_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(user_agents)\n",
        "    }\n",
        "\n",
        "# Domains to Exclude\n",
        "exclude_domains = [\n",
        "    'dictionary.com', 'wiktionary.org', 'merriam-webster.com',\n",
        "    'facebook.com', 'twitter.com', 'vimeo.com', 'youtube.com',\n",
        "    'linkedin.com', 'reddit.com', 'quora.com', 'instagram.com',\n",
        "    'tiktok.com', 'pinterest.com', 'justia.com'\n",
        "]\n",
        "\n",
        "# Keywords to detect dictionary-related content\n",
        "dictionary_keywords = [\n",
        "    \"definition\", \"meaning\", \"dictionary\", \"thesaurus\", \"pronunciation\"\n",
        "]\n",
        "\n",
        "def is_valid_url(url):\n",
        "    return not any(domain in url for domain in exclude_domains)\n",
        "\n",
        "# Bing Search Scraper\n",
        "def bing_search_scrape(company, retries=3, backoff_factor=5):\n",
        "    query = f'\"{company}\"'\n",
        "    url = f\"https://www.bing.com/search?q={quote(query)}\"\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "                results = []\n",
        "\n",
        "                for g in soup.find_all('li', class_='b_algo'):\n",
        "                    link_tag = g.find('a')\n",
        "                    if not link_tag or 'href' not in link_tag.attrs:\n",
        "                        continue\n",
        "\n",
        "                    link = link_tag['href']\n",
        "                    title = g.find('h2').text if g.find('h2') else \"\"\n",
        "                    snippet = g.find('p').text if g.find('p') else \"\"\n",
        "\n",
        "                    if not is_valid_url(link):\n",
        "                        continue\n",
        "\n",
        "                    if title or snippet:\n",
        "                        results.append({\n",
        "                            \"title\": title,\n",
        "                            \"link\": link,\n",
        "                            \"snippet\": snippet\n",
        "                        })\n",
        "\n",
        "                # Return results if found\n",
        "                if len(results) > 0:\n",
        "                    return results\n",
        "                else:\n",
        "                    print(f\"No results for {company}, retrying...\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Failed to fetch results for {company} (Attempt {attempt+1}): {response.status_code}\")\n",
        "\n",
        "        except (RequestException, SSLError) as e:\n",
        "            print(f\"Error scraping Bing for {company} (Attempt {attempt+1}): {e}\")\n",
        "\n",
        "        # Exponential backoff before next retry\n",
        "        time.sleep((attempt + 1) * backoff_factor + random.uniform(2, 5))\n",
        "\n",
        "    # Return empty if all retries fail\n",
        "    return []\n",
        "\n",
        "# Function to check if the content is likely from a dictionary\n",
        "def is_dictionary_page(soup):\n",
        "    # Check title\n",
        "    if soup.title and any(word in soup.title.text.lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check meta description\n",
        "    meta_description = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "    if meta_description and any(word in meta_description.get(\"content\", \"\").lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check h1 or h2 headings\n",
        "    for tag in soup.find_all(['h1', 'h2']):\n",
        "        if any(word in tag.text.lower() for word in dictionary_keywords):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# Extract Relevant Text from URL (Updated)\n",
        "def extract_text_from_url(url, company_name):\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "        if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Filter out dictionary pages\n",
        "            if is_dictionary_page(soup):\n",
        "                return \"Dictionary content skipped\"\n",
        "\n",
        "            # Clean unwanted sections\n",
        "            for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                script.extract()\n",
        "            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "                comment.extract()\n",
        "\n",
        "            # Extract relevant text\n",
        "            relevant_text = \"\"\n",
        "            for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
        "                if company_name.lower() in tag.text.lower():\n",
        "                    relevant_text += tag.text + \" \"\n",
        "\n",
        "            if not relevant_text:\n",
        "                relevant_text = ' '.join(soup.stripped_strings)\n",
        "\n",
        "            return re.sub(r'\\s+', ' ', relevant_text[:2000])\n",
        "        else:\n",
        "            return \"Non-text content skipped\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def analyze_text_with_nlp(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LAW']]\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "    sentiment = 'Positive' if sentiment_score > 0.05 else 'Negative' if sentiment_score < -0.05 else 'Neutral'\n",
        "\n",
        "    return entities, sentiment, sentiment_score\n",
        "\n",
        "def calculate_score_with_reason(text, snippet, company_name):\n",
        "    text_lower = text.lower()\n",
        "    snippet_lower = snippet.lower()\n",
        "    matching_keywords = []\n",
        "    score = score_no_words\n",
        "\n",
        "    # Perform NLP Analysis\n",
        "    entities, sentiment, sentiment_score = analyze_text_with_nlp(text)\n",
        "\n",
        "    # Direct sentence match for precision\n",
        "    def keyword_in_same_sentence(keyword):\n",
        "        sentences = re.split(r'[.!?]', text_lower)\n",
        "        for sentence in sentences:\n",
        "            if company_name.lower() in sentence and keyword in sentence:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Proximity match for broader detection\n",
        "    def keyword_near_company(keyword):\n",
        "        match = re.search(rf\"\\b{keyword}\\b\", text_lower)\n",
        "        if match:\n",
        "            window = text_lower[max(0, match.start() - 500):match.end() + 500]\n",
        "            if company_name.lower() in window:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # High-risk keyword scoring\n",
        "    for keyword in keywords_score_30:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "\n",
        "    # Medium-risk keyword scoring\n",
        "    for keyword in keywords_score_5:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "\n",
        "    # Negative keywords (reduce score)\n",
        "    for keyword in keywords_score_minus_1:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "\n",
        "    # Boost for financial distress mentions\n",
        "    if not matching_keywords:\n",
        "       for term in [\"insolvency\", \"bankruptcy\", \"liquidation\", \"dissolved\"]:\n",
        "        if term in text_lower:\n",
        "            score += 5\n",
        "            matching_keywords.append(term)\n",
        "\n",
        "    # Sentiment-based Adjustment\n",
        "    if sentiment == 'Negative':\n",
        "        score += 2  # Increase score more significantly for negative sentiment\n",
        "    elif sentiment == 'Positive' and score > 5:\n",
        "        score -= 1  # Reduce the score slightly, but not below zero\n",
        "    elif sentiment == 'Neutral':\n",
        "        score += 1  # Slightly increase score for neutral sentiment to reflect uncertainty\n",
        "    return score, matching_keywords or [\"No relevant keywords\"], entities, sentiment\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "# Process Each Company\n",
        "def process_company(company_name):\n",
        "    try:\n",
        "        results = bing_search_scrape(company_name)\n",
        "        company_data = []\n",
        "\n",
        "        # If no results, append a placeholder entry with a score of 0\n",
        "        if not results:\n",
        "            return [{\n",
        "                'company': company_name,\n",
        "                'url': \"N/A\",\n",
        "                'extracted_text': \"No results found\",\n",
        "                'score': 0,\n",
        "                'matched_keywords': \"No relevant keywords\",\n",
        "                'entities': \"N/A\",\n",
        "                'sentiment': \"N/A\"\n",
        "            }]\n",
        "\n",
        "        # Process results if found\n",
        "        for result in results:\n",
        "            url = result['link']\n",
        "            snippet = result['snippet']\n",
        "            extracted_text = extract_text_from_url(url, company_name)\n",
        "\n",
        "            if extracted_text != \"Non-text content skipped\":\n",
        "                extracted_text = clean_text(extracted_text)\n",
        "                score, reasons, entities, sentiment = calculate_score_with_reason(\n",
        "                    extracted_text, snippet, company_name\n",
        "                )\n",
        "                company_data.append({\n",
        "                    'company': company_name,\n",
        "                    'url': url,\n",
        "                    'extracted_text': extracted_text[:300],\n",
        "                    'score': score,\n",
        "                    'matched_keywords': ', '.join(reasons),\n",
        "                    'entities': ', '.join(entities),\n",
        "                    'sentiment': sentiment\n",
        "                })\n",
        "\n",
        "        # If no valid results (filtered by dictionary check), add a placeholder\n",
        "        if not company_data:\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': \"N/A\",\n",
        "                'extracted_text': \"No valid results\",\n",
        "                'score': 0,\n",
        "                'matched_keywords': \"No relevant keywords\",\n",
        "                'entities': \"N/A\",\n",
        "                'sentiment': \"N/A\"\n",
        "            })\n",
        "\n",
        "        return company_data\n",
        "\n",
        "    except Exception:\n",
        "        # Return error status with a score of 5\n",
        "        return [{\n",
        "            \"company\": company_name,\n",
        "            \"url\": \"N/A\",\n",
        "            \"extracted_text\": \"Error occurred\",\n",
        "            \"score\": 0,\n",
        "            \"matched_keywords\": \"Error\",\n",
        "            \"entities\": \"N/A\",\n",
        "            \"sentiment\": \"N/A\"\n",
        "        }]\n",
        "\n",
        "# Multithreading for Efficiency (Batching in groups of 50)\n",
        "data = []\n",
        "batch_size = 50\n",
        "total_companies_processed = 0\n",
        "\n",
        "for i in range(0, len(company_names), batch_size):\n",
        "    batch = company_names[i:i + batch_size]\n",
        "    batch_results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = {executor.submit(process_company, name): name for name in batch}\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                results = future.result()\n",
        "                batch_results.extend(results)\n",
        "            except Exception as e:\n",
        "                print(f\"Error during processing batch: {e}\")\n",
        "\n",
        "    # Extend data with the batch results\n",
        "    data.extend(batch_results)\n",
        "\n",
        "# Track unique companies\n",
        "unique_companies_processed = set()\n",
        "\n",
        "for i in range(0, len(company_names), batch_size):\n",
        "    batch = company_names[i:i + batch_size]\n",
        "    batch_results = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = {executor.submit(process_company, name): name for name in batch}\n",
        "        for future in as_completed(futures):\n",
        "            results = future.result()\n",
        "            batch_results.extend(results)\n",
        "            for entry in results:\n",
        "                unique_companies_processed.add(entry['company'])\n",
        "\n",
        "    # Count processed companies for the current batch\n",
        "    companies_in_batch = len(batch_results)\n",
        "    total_companies_processed = len(unique_companies_processed)\n",
        "\n",
        "    print(f\"Batch {i // batch_size + 1} completed. {companies_in_batch} companies processed in this batch.\")\n",
        "\n",
        "    # Delay between batches to prevent blocking\n",
        "    if i + batch_size < len(company_names):\n",
        "        wait_time = random.randint(45, 75)\n",
        "        print(f\"Waiting {wait_time} seconds before next batch...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "# Final count of all processed companies\n",
        "print(f\"\\nTotal unique companies processed: {total_companies_processed}\")\n",
        "df_results = pd.DataFrame(data)\n",
        "df_results.to_csv('Step_3.2_company_analysis_with_scores.csv', index=False, encoding='utf-8', quoting=csv.QUOTE_ALL, escapechar='\\\\')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YupNqXcZm8zp",
        "outputId": "bd6b19f4-9e60-405e-f99a-a9a01bfc9e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No results for Orangette (L'), retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "No results for JGI-Computers, retrying...\n",
            "No results for Just Communication, retrying...\n",
            "No results for Just Communication, retrying...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "No results for Azet, retrying...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Eprivate SPRL, retrying...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "No results for Palais De Chine [LE PALAIS DE CHINE], retrying...\n",
            "No results for Dog's Club, retrying...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for Fantasia, retrying...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "No results for Het Podium, retrying...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "No results for Vermeulen/Christ, retrying...\n",
            "No results for JMP Cars, retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "No results for International Lashing Systems nv [INTERNATIONAL LASHING SYSTEMS - MARITIME CONSOLIDATORS BELGIUM], retrying...\n",
            "Batch 1 completed. 290 companies processed in this batch.\n",
            "Waiting 65 seconds before next batch...\n",
            "Batch 2 completed. 334 companies processed in this batch.\n",
            "Waiting 62 seconds before next batch...\n",
            "No results for Verbist/Paul, retrying...\n",
            "No results for Verbist/Paul, retrying...\n",
            "No results for Hanson Aggregates Belgium nv, retrying...\n",
            "Batch 3 completed. 256 companies processed in this batch.\n",
            "Waiting 60 seconds before next batch...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for Lorsom/Marc, retrying...\n",
            "No results for R.S.B., retrying...\n",
            "No results for Brabo CVBA, retrying...\n",
            "Batch 4 completed. 294 companies processed in this batch.\n",
            "Waiting 46 seconds before next batch...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Oh ! La ! La ! Boulangerie [OH] LA] LA] BOULANGERIE SPRL], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "No results for Delvith [DELVITH AG], retrying...\n",
            "Batch 5 completed. 285 companies processed in this batch.\n",
            "Waiting 48 seconds before next batch...\n",
            "No results for Sans Doute, retrying...\n",
            "Batch 6 completed. 296 companies processed in this batch.\n",
            "Waiting 58 seconds before next batch...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Rottiers No?lla, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "No results for Elizabal/Lydie, retrying...\n",
            "Batch 7 completed. 311 companies processed in this batch.\n",
            "Waiting 62 seconds before next batch...\n",
            "No results for Proot/Patrick, retrying...\n",
            "No results for Proot/Patrick, retrying...\n",
            "Batch 8 completed. 304 companies processed in this batch.\n",
            "Waiting 53 seconds before next batch...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for Pauwelyn Koen, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "No results for G.M.T. Belgium SA, retrying...\n",
            "No results for Detaille/Jo?lle, retrying...\n",
            "Batch 9 completed. 294 companies processed in this batch.\n",
            "Waiting 59 seconds before next batch...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "No results for Economic [ECONOMIC BVBA], retrying...\n",
            "Batch 10 completed. 292 companies processed in this batch.\n",
            "\n",
            "Total unique companies processed: 500\n"
          ]
        }
      ]
    }
  ]
}