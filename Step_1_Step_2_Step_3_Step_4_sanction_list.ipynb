{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NataKrj/AI-project-2024/blob/main/Step_1_Step_2_Step_3_Step_4_sanction_list.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install pandas\n",
        "!pip install fuzzywuzzy\n",
        "!pip install selenium\n",
        "!pip install spacy nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFZ4Whe53TKl",
        "outputId": "492686c0-9df3-4071-ef9e-bcffc2516cda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.12.14)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32009 sha256=39419eb3a4ca5f1bb3c0af80a69f42842afd6fb71803ca42df6c78a5462b970c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Installing collected packages: fuzzywuzzy\n",
            "Successfully installed fuzzywuzzy-0.18.0\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.27.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.27.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.12.14)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
            "Downloading selenium-4.27.1-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.27.0-py3-none-any.whl (481 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m481.7/481.7 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sortedcontainers, wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.27.1 sortedcontainers-2.4.0 trio-0.27.0 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Step -Sanction list check"
      ],
      "metadata": {
        "id": "ZKnITnhz0rKj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhxrU0ZFUugS",
        "outputId": "9a81b8de-856e-45b3-cd0e-cb0f3d5a0050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sanctioned names saved to 'sanction_list.csv'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# URLs for the CSV files\n",
        "url1 = 'https://www.treasury.gov/ofac/downloads/sdn.csv'\n",
        "url2 = 'https://www.treasury.gov/ofac/downloads/consolidated/cons_alt.csv'\n",
        "\n",
        "# Read data from the first CSV file\n",
        "df1 = pd.read_csv(url1, on_bad_lines='skip')\n",
        "sanction_list_url1 = df1.iloc[:, 1].dropna().unique()\n",
        "\n",
        "# Read data from the second CSV file\n",
        "df2 = pd.read_csv(url2, on_bad_lines='skip')\n",
        "sanction_list_url2 = df2.iloc[:, 3].dropna().unique()\n",
        "\n",
        "# Combine the names from both CSV files\n",
        "sanction_list = list(set(sanction_list_url1) | set(sanction_list_url2))\n",
        "\n",
        "# Create a DataFrame from the combined names\n",
        "sanction_list_df = pd.DataFrame({'Sanctioned Names': sanction_list})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "sanction_list_df.to_csv('sanction_list.csv', index=False)\n",
        "\n",
        "print(\"Sanctioned names saved to 'sanction_list.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from fuzzywuzzy import fuzz\n",
        "\n",
        "# Paths to the files\n",
        "uploaded_file_path = 'BELGIUM_companies.csv'\n",
        "sanction_list_file_path = 'sanction_list.csv'\n",
        "\n",
        "# Load the companies file\n",
        "companies_df = pd.read_csv(uploaded_file_path, low_memory=False, encoding='utf-8')\n",
        "company_names = companies_df['OriginalCompanyName'].str.lower()[0:500].tolist()  # Processing the first 500 companies\n",
        "\n",
        "# Load the sanction list\n",
        "sanction_list_df = pd.read_csv(sanction_list_file_path)\n",
        "\n",
        "# Normalize the sanction list for case-insensitive matching\n",
        "sanctioned_names = sanction_list_df['Sanctioned Names'].str.lower().tolist()\n",
        "\n",
        "# Function for approximate matching\n",
        "def approximate_match(name, sanctioned_names, threshold=85):\n",
        "    \"\"\" Check if a name approximately matches any sanctioned name.\n",
        "    : name: Name to match\n",
        "    :sanctioned_names: List of sanctioned names\n",
        "    :threshold: Minimum similarity score for a match\n",
        "    : 46 if a match is found, 0 otherwise\n",
        "    \"\"\"\n",
        "    name = name.lower()\n",
        "    for sanctioned_name in sanctioned_names:\n",
        "        similarity = fuzz.ratio(name, sanctioned_name)\n",
        "        if similarity >= threshold:\n",
        "            return 46  # Match found\n",
        "    return 0  # No match\n",
        "\n",
        "# Evaluate if company names approximately match any name in the sanction list\n",
        "companies_df['Score_Step_1'] = companies_df['OriginalCompanyName'].apply(\n",
        "    lambda name: approximate_match(name, sanctioned_names)\n",
        ")\n",
        "\n",
        "# Save the updated  new file\n",
        "output_file_path = 'Step_1_evaluated_companies.xlsx'\n",
        "companies_df.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"Evaluation complete with approximate matching. Results saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "udXl_T6-2r1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II STEP- Company status/ active check"
      ],
      "metadata": {
        "id": "5eVlnOsGAtwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datetime import datetime\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "from time import sleep\n",
        "from multiprocessing import Pool\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 45)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "        \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "        \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "    ]\n",
        "\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 15\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    # Load the Excel file\n",
        "    uploaded_file_path = 'BELGIUM_companies'\n",
        "    company_list = pd.read_excel(uploaded_file_path)['OriginalCompanyName']\n",
        "\n",
        "    num_workers = 5\n",
        "\n",
        "    # Split the list of companies into chunks for multiprocessing\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "\n",
        "    # Combine all results\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "\n",
        "    # Define the scoring dictionary\n",
        "    status_scores = {\n",
        "        \"ENT LP Active\": 1,\n",
        "        \"ENT LP Stopped\": 5,\n",
        "        \"error\": 2,\n",
        "        \"EU Active\": 1,\n",
        "        \"EU Stopped\": 5,\n",
        "        \"No result found for this search term\": 2,\n",
        "        \"not found in KBO data table\": 2\n",
        "    }\n",
        "\n",
        "    # Map the 'Status' column to scores based on the dictionary\n",
        "    result_df['Score'] = result_df['Status'].map(status_scores).fillna(0)\n",
        "\n",
        "    # Save the updated DataFrame to a new CSV file\n",
        "    result_df.to_csv('Step_2_company_status_report_with_scores.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtYJWL3CL-E",
        "outputId": "e0c13b36-cf14-407b-872f-ef5eba34e760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start time: 2024-12-23 22:35:15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No result for Zwick Roell Belux CV\n",
            "No result for Van Laer-Mazet/Chris\n",
            "No result for Brugs Motoren Bedrijf nv\n",
            "No result for Zzlite\n",
            "End time: 2024-12-23 22:37:33\n",
            "Total time taken: 0:02:17.728790\n",
            "Total successfully found statuses: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "from multiprocessing import Pool\n",
        "import numpy as np\n",
        "from time import sleep\n",
        "\n",
        "def process_companies(company_chunk):\n",
        "    options = webdriver.ChromeOptions()\n",
        "    options.add_argument('--headless')\n",
        "    options.add_argument('--no-sandbox')\n",
        "    options.add_argument('--disable-dev-shm-usage')\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    wait = WebDriverWait(driver, 45)\n",
        "\n",
        "    base_url = \"https://kbopub.economie.fgov.be/kbopub/zoeknaamfonetischform.html?lang=en\"\n",
        "    result_chunk = []\n",
        "    successful_count = 0\n",
        "\n",
        "    company_types = [\n",
        "    \"VZW\", \"BVBA\", \"BV\", \"NV\", \"CV\", \"CVBA\", \"SPRL\", \"SCRL\", \"ASBL\",\n",
        "    \"Comm.V\", \"SComm\", \"VOF\", \"SNC\", \"GIE\", \"AIE\", \"SE\", \"Partnership\"\n",
        "]\n",
        "    def clean_company_name(company_name):\n",
        "        return re.sub(r'\\b(?:' + '|'.join(company_types) + r')\\b', '', company_name, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    sleep_time = 15\n",
        "    for company_name in company_chunk:\n",
        "        try:\n",
        "            clean_name = clean_company_name(company_name)\n",
        "            driver.get(base_url)\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "            sleep(sleep_time)\n",
        "            search_box = wait.until(EC.presence_of_element_located((By.ID, \"searchWord\")))\n",
        "            search_box.clear()\n",
        "            search_box.send_keys(clean_name)\n",
        "\n",
        "            checkbox = driver.find_element(By.ID, \"filterEnkelActieve\")\n",
        "            if checkbox.is_selected():\n",
        "                checkbox.click()\n",
        "\n",
        "            search_button = wait.until(EC.element_to_be_clickable((By.NAME, \"actionNPRP\")))\n",
        "            search_button.click()\n",
        "            wait.until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
        "\n",
        "            try:\n",
        "                page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "                if \"no result found for this search term.\".lower() in page_text.lower():\n",
        "                    print(f\"No result for {company_name}\")\n",
        "                    result_chunk.append({\n",
        "                        'OriginalCompanyName': company_name,\n",
        "                        'CleanedCompanyName': clean_name,\n",
        "                        'Status': \"No result found for this search term\",\n",
        "                        'Timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                    })\n",
        "                    continue\n",
        "            except NoSuchElementException:\n",
        "                pass\n",
        "\n",
        "            rows = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, '#onderneminglistfonetisch tbody tr')))\n",
        "            status = \"not found in KBO data table\"\n",
        "            timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            for row in rows:\n",
        "                name_cell = row.find_element(By.CLASS_NAME, 'benaming').text.strip()\n",
        "                if name_cell.lower() == clean_name.lower():\n",
        "                    status_cell = row.find_elements(By.TAG_NAME, 'td')[1].text.strip()\n",
        "                    status = re.sub(r'\\s+', ' ', status_cell).strip()\n",
        "                    successful_count += 1\n",
        "                    break\n",
        "\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': status,\n",
        "                'Timestamp': timestamp\n",
        "            })\n",
        "\n",
        "        except (NoSuchElementException, TimeoutException, Exception) as e:\n",
        "            print(f\"Failed to process {company_name}\")\n",
        "            result_chunk.append({\n",
        "                'OriginalCompanyName': company_name,\n",
        "                'CleanedCompanyName': clean_name,\n",
        "                'Status': \"error\",\n",
        "                'Timestamp': \"N/A\"\n",
        "            })\n",
        "\n",
        "    driver.quit()\n",
        "    return result_chunk, successful_count\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    start_time = datetime.now()\n",
        "    print(f\"Start time: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "    company_list = pd.read_csv('random_sample_2500_5.csv', encoding='latin-1', sep=',')['Name']\n",
        "    num_workers = 5\n",
        "\n",
        "    company_chunks = np.array_split(company_list, num_workers)\n",
        "    with Pool(num_workers) as pool:\n",
        "        results = pool.map(process_companies, company_chunks)\n",
        "    all_results = [item[0] for item in results]\n",
        "    successful_count = sum(item[1] for item in results)\n",
        "    result_df = pd.DataFrame([item for sublist in all_results for item in sublist])\n",
        "    result_df.to_csv('company_status_random_sample_2500_5.csv', index=False)\n",
        "\n",
        "    end_time = datetime.now()\n",
        "    print(f\"End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"Total time taken: {end_time - start_time}\")\n",
        "    print(f\"Total successfully found statuses: {successful_count}\")"
      ],
      "metadata": {
        "id": "oMnlQnvXbr3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III STEP- web scraping"
      ],
      "metadata": {
        "id": "a4KzImkXOlZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 STEP - web scraping using API"
      ],
      "metadata": {
        "id": "EsxKZdSkHjtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from googleapiclient.discovery import build\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "\n",
        "# Set up your API keys\n",
        "google_api_key = 'xxx'  # Replace with your Google API key\n",
        "google_cse_id = 'xxx'  # Replace with your Custom Search Engine ID\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "#df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "df = pd.read_excel('BELGIUM_companies_short.xlsx', engine='openpyxl')\n",
        "#company_names = df['name'][0:20].tolist()  # Processing the first 20 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"criminal case\", \"arrested\"\n",
        "]\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\", \"insolvency\", \"violation\", \"debt\", \"blackmail\"\n",
        "]\n",
        "keywords_score_minus_1 = [\"stock\"]  # Negative scoring for \"stock\"\n",
        "\n",
        "score_no_words = 0\n",
        "\n",
        "def google_search(search_term, api_key, cse_id, start_index=1):\n",
        "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
        "    try:\n",
        "        res = service.cse().list(q=search_term, cx=cse_id, start=start_index).execute()\n",
        "        return res.get('items', [])\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to search for {search_term} with error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_text_from_url(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0',\n",
        "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
        "        'Accept-Language': 'en-US,en;q=0.5'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, verify=False, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            if 'text/html' in response.headers.get('Content-Type', ''):\n",
        "                soup = BeautifulSoup(response.text, 'html.parser')\n",
        "                for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                    script.extract()\n",
        "                for comment in soup.findAll(text=lambda text: isinstance(text, Comment)):\n",
        "                    comment.extract()\n",
        "                text = ' '.join(soup.stripped_strings)\n",
        "                return text\n",
        "            else:\n",
        "                return \"Non-text content skipped\"\n",
        "        else:\n",
        "            return \"\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple whitespace with single space\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def calculate_score(text):\n",
        "    \"\"\"\n",
        "    Determine the score for a given text based on keyword matching.\n",
        "    \"\"\"\n",
        "    text_lower = text.lower()\n",
        "    if any(keyword in text_lower for keyword in keywords_score_30):\n",
        "        return 30\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_5):\n",
        "        return 5\n",
        "    elif any(keyword in text_lower for keyword in keywords_score_minus_1):\n",
        "        return -1\n",
        "    elif text.strip() == \"\":\n",
        "        return score_no_words\n",
        "    else:\n",
        "        return score_no_words\n",
        "\n",
        "def process_company(company_name):\n",
        "    results = google_search(company_name, google_api_key, google_cse_id)\n",
        "    company_data = []\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        extracted_text = extract_text_from_url(url)\n",
        "        if extracted_text != \"Non-text content skipped\":\n",
        "            extracted_text = clean_text(extracted_text)\n",
        "            score = calculate_score(extracted_text)\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': extracted_text,\n",
        "                'score': score\n",
        "            })\n",
        "        else:\n",
        "            company_data.append({\n",
        "                'company': company_name,\n",
        "                'url': url,\n",
        "                'extracted_text': \"Skipped due to non-text content\",\n",
        "                'score': score_no_words\n",
        "            })\n",
        "    return company_data\n",
        "\n",
        "# Use ThreadPoolExecutor to process companies in parallel\n",
        "data = []\n",
        "with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "    futures = {executor.submit(process_company, name): name for name in company_names}\n",
        "    for future in as_completed(futures):\n",
        "        data.extend(future.result())\n",
        "\n",
        "# Convert list of dicts to DataFrame\n",
        "df_results = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file with proper encoding and escaping\n",
        "output_file_path = 'Step_3.1_company_analysis_with_scores.csv'  # Replace with desired file path\n",
        "df_results.to_csv(output_file_path, index=False, escapechar='\\\\', encoding='utf-8', quoting=csv.QUOTE_ALL)\n",
        "\n",
        "print(f\"Data saved to {output_file_path}.\")"
      ],
      "metadata": {
        "id": "PEj-5ZNlOiBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 STEP - web scraping using BeautifulSoup"
      ],
      "metadata": {
        "id": "jY9e9kMIH0we"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup, Comment\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import csv\n",
        "import re\n",
        "from requests.exceptions import RequestException, SSLError\n",
        "import time\n",
        "from urllib.parse import quote\n",
        "import random\n",
        "import spacy\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load the NLP model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "\n",
        "# Load the CSV file to get company names\n",
        "#df = pd.read_excel('BELGIUM_companies_short.xlsx', engine='openpyxl')\n",
        "df = pd.read_csv('Offshore Leaks-entities.csv', low_memory=False, encoding='utf-8')\n",
        "company_names = df['Name'][0:20].tolist()  # Processing the first 20 companies\n",
        "\n",
        "# Keywords and associated scores\n",
        "keywords_score_30 = [\n",
        "    \"sanctions\", \"criminal\", \"crime\", \"corruption\", \"shell company\", \"offshore\",\n",
        "    \"criminal case\", \"arrested\", \"fraud\", \"money laundering\",\n",
        "    \"embezzlement\", \"terrorism financing\", \"bribery\", \"tax evasion\",\n",
        "    \"illicit funds\", \"smuggling\", \"seized assets\", \"fines\",\n",
        "    \"indictment\", \"prosecuted\", \"wanted\", \"scam\", \"scandal\"\n",
        "]\n",
        "\n",
        "keywords_score_5 = [\n",
        "    \"court\", \"accusation\", \"penalty\", \"investigation\",\n",
        "    \"insolvency\", \"violation\", \"debt\", \"blackmail\", \"lawsuit\",\n",
        "    \"default\", \"litigation\", \"settlement\", \"audit\", \"suspicious\",\n",
        "    \"foreclosure\", \"dispute\", \"breach\", \"illegal transaction\",\n",
        "    \"arbitration\", \"compliance failure\", \"tax fraud\"\n",
        "]\n",
        "\n",
        "keywords_score_minus_1 = [\"stock\"]  # Negative scoring\n",
        "\n",
        "score_no_words = 0\n",
        "\n",
        "# User-Agent Rotation\n",
        "user_agents = [\n",
        "\n",
        "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)\",\n",
        "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "    \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X)\",\n",
        "    \"Mozilla/5.0 (iPad; CPU OS 13_2 like Mac OS X)\"\n",
        "]\n",
        "\n",
        "def random_headers():\n",
        "    return {\n",
        "        \"User-Agent\": random.choice(user_agents)\n",
        "    }\n",
        "\n",
        "# Domains to Exclude\n",
        "exclude_domains = [\n",
        "    'dictionary.com', 'wiktionary.org', 'merriam-webster.com',\n",
        "    'facebook.com', 'twitter.com', 'vimeo.com', 'youtube.com',\n",
        "    'linkedin.com', 'reddit.com', 'quora.com', 'instagram.com',\n",
        "    'tiktok.com', 'pinterest.com', 'justia.com'\n",
        "]\n",
        "\n",
        "def is_valid_url(url):\n",
        "    return not any(domain in url for domain in exclude_domains)\n",
        "\n",
        "# Bing Search Scraper\n",
        "def bing_search_scrape(company):\n",
        "    query = f'\"{company}\"'\n",
        "    url = f\"https://www.bing.com/search?q={quote(query)}\"\n",
        "    time.sleep(random.uniform(3, 7))\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to fetch results for {company}: {response.status_code}\")\n",
        "            return []\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        results = []\n",
        "\n",
        "        for g in soup.find_all('li', class_='b_algo'):\n",
        "            link_tag = g.find('a')\n",
        "            if not link_tag or 'href' not in link_tag.attrs:\n",
        "                continue\n",
        "\n",
        "            link = link_tag['href']\n",
        "            title = g.find('h2').text if g.find('h2') else \"\"\n",
        "            snippet = g.find('p').text if g.find('p') else \"\"\n",
        "\n",
        "            if not is_valid_url(link):\n",
        "                continue\n",
        "\n",
        "            if title or snippet:\n",
        "                results.append({\n",
        "                    \"title\": title,\n",
        "                    \"link\": link,\n",
        "                    \"snippet\": snippet\n",
        "                })\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping Bing for {company}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Keywords to detect dictionary-related content\n",
        "dictionary_keywords = [\n",
        "    \"definition\", \"meaning\", \"dictionary\", \"thesaurus\", \"pronunciation\"\n",
        "]\n",
        "\n",
        "# Function to check if the content is likely from a dictionary\n",
        "def is_dictionary_page(soup):\n",
        "    # Check title\n",
        "    if soup.title and any(word in soup.title.text.lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check meta description\n",
        "    meta_description = soup.find(\"meta\", {\"name\": \"description\"})\n",
        "    if meta_description and any(word in meta_description.get(\"content\", \"\").lower() for word in dictionary_keywords):\n",
        "        return True\n",
        "\n",
        "    # Check h1 or h2 headings\n",
        "    for tag in soup.find_all(['h1', 'h2']):\n",
        "        if any(word in tag.text.lower() for word in dictionary_keywords):\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "# Extract Relevant Text from URL (Updated)\n",
        "def extract_text_from_url(url, company_name):\n",
        "    try:\n",
        "        response = requests.get(url, headers=random_headers(), timeout=10)\n",
        "        if response.status_code == 200 and 'text/html' in response.headers.get('Content-Type', ''):\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Filter out dictionary pages\n",
        "            if is_dictionary_page(soup):\n",
        "                return \"Dictionary content skipped\"\n",
        "\n",
        "            # Clean unwanted sections\n",
        "            for script in soup([\"script\", \"style\", \"header\", \"footer\", \"form\", \"nav\"]):\n",
        "                script.extract()\n",
        "            for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
        "                comment.extract()\n",
        "\n",
        "            # Extract relevant text\n",
        "            relevant_text = \"\"\n",
        "            for tag in soup.find_all(['h1', 'h2', 'h3', 'p']):\n",
        "                if company_name.lower() in tag.text.lower():\n",
        "                    relevant_text += tag.text + \" \"\n",
        "\n",
        "            if not relevant_text:\n",
        "                relevant_text = ' '.join(soup.stripped_strings)\n",
        "\n",
        "            return re.sub(r'\\s+', ' ', relevant_text[:2000])\n",
        "        else:\n",
        "            return \"Non-text content skipped\"\n",
        "    except (RequestException, SSLError) as e:\n",
        "        return f\"Request failed for {url}: {e}\"\n",
        "\n",
        "def analyze_text_with_nlp(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Named Entity Recognition (NER)\n",
        "    entities = [ent.text for ent in doc.ents if ent.label_ in ['ORG', 'PERSON', 'GPE', 'LAW']]\n",
        "\n",
        "    # Sentiment Analysis\n",
        "    sentiment_score = sia.polarity_scores(text)['compound']\n",
        "    sentiment = 'Positive' if sentiment_score > 0.05 else 'Negative' if sentiment_score < -0.05 else 'Neutral'\n",
        "\n",
        "    return entities, sentiment, sentiment_score\n",
        "\n",
        "def calculate_score_with_reason(text, snippet, company_name):\n",
        "    text_lower = text.lower()\n",
        "    snippet_lower = snippet.lower()\n",
        "    matching_keywords = []\n",
        "    score = score_no_words\n",
        "\n",
        "    # Perform NLP Analysis\n",
        "    entities, sentiment, sentiment_score = analyze_text_with_nlp(text)\n",
        "\n",
        "    # Direct sentence match for precision\n",
        "    def keyword_in_same_sentence(keyword):\n",
        "        sentences = re.split(r'[.!?]', text_lower)\n",
        "        for sentence in sentences:\n",
        "            if company_name.lower() in sentence and keyword in sentence:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # Proximity match for broader detection\n",
        "    def keyword_near_company(keyword):\n",
        "        match = re.search(rf\"\\b{keyword}\\b\", text_lower)\n",
        "        if match:\n",
        "            window = text_lower[max(0, match.start() - 500):match.end() + 500]\n",
        "            if company_name.lower() in window:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    # High-risk keyword scoring\n",
        "    for keyword in keywords_score_30:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 30\n",
        "\n",
        "    # Medium-risk keyword scoring\n",
        "    for keyword in keywords_score_5:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score += 5\n",
        "\n",
        "    # Negative keywords (reduce score)\n",
        "    for keyword in keywords_score_minus_1:\n",
        "        if keyword_in_same_sentence(keyword):\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "        elif keyword_near_company(keyword) or keyword in snippet_lower:\n",
        "            matching_keywords.append(keyword)\n",
        "            score -= 1\n",
        "\n",
        "    # Boost for financial distress mentions\n",
        "    if not matching_keywords:\n",
        "       for term in [\"insolvency\", \"bankruptcy\", \"liquidation\", \"dissolved\"]:\n",
        "        if term in text_lower:\n",
        "            score += 5\n",
        "            matching_keywords.append(term)\n",
        "\n",
        "    # Sentiment-based Adjustment\n",
        "    if sentiment == 'Negative':\n",
        "        score += 5  # Increase score for negative sentiment\n",
        "    elif sentiment == 'Positive':\n",
        "        score = 0  # Reduce score slightly for positive sentiment\n",
        "\n",
        "    return score, matching_keywords or [\"No relevant keywords\"], entities, sentiment\n",
        "\n",
        "# Process Each Company\n",
        "def process_company(company_name):\n",
        "    results = bing_search_scrape(company_name)\n",
        "    company_data = []\n",
        "\n",
        "    for result in results:\n",
        "        url = result['link']\n",
        "        snippet = result['snippet']\n",
        "        extracted_text = extract_text_from_url(url, company_name)\n",
        "\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "def process_company(company_name):\n",
        "    try:\n",
        "        results = bing_search_scrape(company_name)\n",
        "        company_data = []\n",
        "\n",
        "        for result in results:\n",
        "            url = result['link']\n",
        "            snippet = result['snippet']\n",
        "            extracted_text = extract_text_from_url(url, company_name)\n",
        "\n",
        "            if extracted_text != \"Non-text content skipped\":\n",
        "                extracted_text = clean_text(extracted_text)  # Clean the extracted text\n",
        "                score, reasons, entities, sentiment = calculate_score_with_reason(\n",
        "                    extracted_text, snippet, company_name\n",
        "                )\n",
        "                company_data.append({\n",
        "                    'company': company_name,\n",
        "                    'url': url,\n",
        "                    'extracted_text': extracted_text[:300],\n",
        "                    'score': score,\n",
        "                    'matched_keywords': ', '.join(reasons),\n",
        "                    'entities': ', '.join(entities),\n",
        "                    'sentiment': sentiment\n",
        "                })\n",
        "        return company_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {company_name}: {e}\")\n",
        "        return [{\n",
        "            \"company\": company_name,\n",
        "            \"url\": \"N/A\",\n",
        "            \"extracted_text\": \"Error\",\n",
        "            \"score\": 0,\n",
        "            \"matched_keywords\": \"Error\",\n",
        "            \"entities\": \"N/A\",\n",
        "            \"sentiment\": \"N/A\"\n",
        "        }]\n",
        "\n",
        "# Multithreading for Efficiency (Batching in groups of 100)\n",
        "data = []\n",
        "batch_size = 100\n",
        "\n",
        "for i in range(0, len(company_names), batch_size):\n",
        "    batch = company_names[i:i + batch_size]\n",
        "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "        futures = {executor.submit(process_company, name): name for name in batch}\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                data.extend(future.result())\n",
        "            except Exception as e:\n",
        "                print(f\"Error during processing batch: {e}\")\n",
        "\n",
        "    # Delay between batches to prevent blocking\n",
        "    if i + batch_size < len(company_names):\n",
        "        wait_time = random.randint(45, 75)\n",
        "        print(f\"Batch {i // batch_size + 1} completed. Waiting {wait_time} seconds before next batch...\")\n",
        "        time.sleep(wait_time)\n",
        "\n",
        "df_results = pd.DataFrame(data)\n",
        "df_results.to_csv('Step_3.2.2_company_analysis_with_scores.csv',\n",
        "                  index=False, encoding='utf-8',\n",
        "                  quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
        "\n",
        "print(\"Data saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YupNqXcZm8zp",
        "outputId": "c00a19f0-393d-4c07-d164-5cd5eca57be7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# IV STEP- companies scoring\n",
        "\n"
      ],
      "metadata": {
        "id": "N_Vp8mfROiuj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data files\n",
        "belgium_companies = pd.read_excel('BELGIUM_companies_short.xlsx')\n",
        "step_1 = pd.read_excel('Step_1_evaluated_companies.xlsx')\n",
        "step_2 = pd.read_csv('Step_2_company_status_report_with_scores.csv')\n",
        "step_3 = pd.read_csv('Step_3.2_company_analysis_with_scores.csv')\n",
        "\n",
        "# Standardize company name columns\n",
        "step_1.rename(columns={'Name': 'company_name'}, inplace=True)\n",
        "step_2.rename(columns={'OriginalCompanyName': 'company_name'}, inplace=True)\n",
        "step_3.rename(columns={'company': 'company_name'}, inplace=True)\n",
        "belgium_companies.rename(columns={'Name': 'company_name'}, inplace=True)\n",
        "\n",
        "# Combine all score files\n",
        "all_scores = pd.concat([step_1[['company_name', 'Score_Step_1']],\n",
        "                        step_2[['company_name', 'Score']],\n",
        "                        step_3[['company_name', 'score']].rename(columns={'score': 'Score'})],\n",
        "                       ignore_index=True)\n",
        "\n",
        "# Filter for companies in the Belgium list\n",
        "filtered_scores = all_scores[all_scores['company_name'].isin(belgium_companies['company_name'])]\n",
        "\n",
        "# Group by company and sum scores\n",
        "total_scores = filtered_scores.groupby('company_name')['Score'].sum().reset_index()\n",
        "\n",
        "# Apply risk level based on total score\n",
        "def assign_risk_level(score):\n",
        "    if score > 30:\n",
        "        return 'prohibited'\n",
        "    elif 7 <= score <= 30:\n",
        "        return 'high'\n",
        "    elif 1 <= score <= 6:\n",
        "        return 'medium'\n",
        "    elif score < 1:\n",
        "        return 'low'\n",
        "    else:\n",
        "        return 'no risk'\n",
        "\n",
        "# Assign risk levels\n",
        "total_scores['risk_level'] = total_scores['Score'].apply(assign_risk_level)\n",
        "\n",
        "# Save results to CSV\n",
        "total_scores.to_csv('Step_4_company_risk_scores.csv', index=False)\n",
        "print(\"Risk scoring completed and saved as 'Step_4_company_risk_scores.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uylK4_6alYZ9",
        "outputId": "db60e3b1-266c-4282-cb7d-14f7470392d1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Risk scoring completed and saved as 'Step_4_company_risk_scores.csv'\n"
          ]
        }
      ]
    }
  ]
}